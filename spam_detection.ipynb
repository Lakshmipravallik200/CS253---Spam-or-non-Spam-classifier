{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam_detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Classification\n",
        "\n",
        "Classification refers to categorizing the given data into classes. For example,\n",
        "- Given an image of hand-written character, identifying the character (multi-class classification)\n",
        "- Given an image, annotating it with all the objects present in the image (multi-label classification)\n",
        "- Classifying an email as spam or non-spam (binary classification)\n",
        "- Classifying a tumor as benign or malignant and so on\n",
        "\n",
        "In this assignment, we will be building a classifier to classify emails as spam or non-spam. We will be using the Kaggle dataset [Spam or Not Spam Dataset](https://www.kaggle.com/datasets/ozlerhakan/spam-or-not-spam-dataset?resource=download) for this task. \n",
        "\n",
        "**Note**: You cannot load any libraries other than the mentioned ones.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BKCrcukdE8Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pre-processing\n",
        "The first step in every machine learning algorithm is to process the raw data in some meaningful representations. We will be using the [Bag-of-Words](https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971) representation to process the text. It comprises of following steps:\n",
        "\n",
        "- Process emails line-by-line to extract all the words.\n",
        "- Replace extracted words by their stem (root) word. This is known as stemming and lematization.\n",
        "- Remove stop words like and, or, is, am, and so on.\n",
        "- Assign a unique index to each word. This forms the vocabulary.\n",
        "- Represent each email as a binary vector of length equal to the size of the vocabulary such that the $i^{th}$ element of the vector is 1 iff the $i^th$ word is present in the email.\n",
        "\n",
        "Here we provide you with the function signature along with the expected functionality. You are expected to complete them accordingly. "
      ],
      "metadata": {
        "id": "Irp6f7_2crfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# takes an email as an argument\n",
        "# read email line-by-line and extract all the words\n",
        "# return list of extracted words\n",
        "def read_email(email):\n",
        "  words=email.split(' ')\n",
        "  return words\n",
        "  \n",
        "# takes a list of words as an argument\n",
        "# replace each word by their stem word\n",
        "# return list of stem words\n",
        "def stemming(processed):\n",
        "  stem_words=[]\n",
        "  ps=nltk.PorterStemmer()\n",
        "  for x in processed:\n",
        "    s=''\n",
        "    for term in x.split():\n",
        "      s=s+ps.stem(term)+' '\n",
        "    stem_words.append(s)\n",
        "  return stem_words\n",
        "\n",
        "# takes a list of stem-words as an argument\n",
        "# remove stop words\n",
        "# return list of stem words after removing stop words\n",
        "def remove_stop_words(email):\n",
        "  stem_no_stop_words=[]\n",
        "  ss=[]\n",
        "  for i in email:\n",
        "    processed=i.replace(r'[^\\w\\d\\s]',' ')\n",
        "    processed=processed.replace(r'\\s+',' ')\n",
        "    stem_no_stop_words.append(processed.replace(r'^\\s+|\\s+?$','').lower())\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  for x in stem_no_stop_words:\n",
        "    s=''\n",
        "    for term in x.split():\n",
        "      if term not in stop_words:\n",
        "        s=s+(term)+' '\n",
        "    ss.append(s)\n",
        "  return ss\n",
        "\n",
        "# takes a list of stem-words as an argument\n",
        "# add new words to the vocabulary and assign a unique index to them\n",
        "# returns new vocabulary\n",
        "def build_vocabulary(final_processed):\n",
        "  all_words=[]\n",
        "  for message in final_processed:\n",
        "    words=word_tokenize(message)\n",
        "    for w in words:\n",
        "      all_words.append(w)\n",
        "  all_words=nltk.FreqDist(all_words)\n",
        "  return list(all_words.keys())[:1500]\n",
        "\n",
        "# takes a list of stem-words and vocabulary as an argument\n",
        "# returns bow representation\n",
        "def get_bow(message,word_features):\n",
        "  words=word_tokenize(message)\n",
        "  features={}\n",
        "  for word in word_features:\n",
        "    features[word]=(word in words)\n",
        "  return features\n",
        "\n",
        "# read the entire dataset\n",
        "# convert emails to bow and maintain their labels\n",
        "# call function text_to_bow()\n",
        "from csv import reader\n",
        "def read_data(filename):\n",
        "  file = open(filename,\"r\") # Open file in read mode\n",
        "  lines = reader(file)\n",
        "  data = list(lines)\n",
        "  return data\n",
        "\n",
        "\n",
        "  #messages_bow = CountVectorizer(analyzer=process_text).fit_transform(df['email'])\n",
        "    \n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "wIlTOyyPn-mN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of dataset\n",
        "import sys\n",
        "import csv\n",
        " \n",
        "csv.field_size_limit(sys.maxsize)\n",
        "filename = \"/content/spam_or_not_spam.csv\"\n",
        "data = read_data(filename)\n",
        "email=[]\n",
        "label=[]\n",
        "for i in range(1,len(data)):\n",
        "  if i==2967:\n",
        "    continue \n",
        "  else:\n",
        "    email.append(data[i][0])\n",
        "    label.append(data[i][1])\n",
        "def get_label():\n",
        "  return label\n",
        "nltk.download('stopwords')\n",
        "processed=remove_stop_words(email)\n",
        "final_processed=stemming(processed)\n",
        "nltk.download('punkt')\n",
        "word_features=build_vocabulary(final_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv6JvBN_SBya",
        "outputId": "3653ad20-b6b4-4673-a2aa-a9b8a7266456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages=list(zip(final_processed,label))\n",
        "#defined seed for reproducability\n",
        "seed=1\n",
        "np.random.seed=seed \n",
        "np.random.shuffle(messages)\n",
        "feature_sets=[(get_bow(t,word_features),l) for (t,l) in messages]\n",
        "\n"
      ],
      "metadata": {
        "id": "claDb-xmNxj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLvgDXKD2tEp",
        "outputId": "1ae6216e-1584-4d12-f60e-3e4a40551547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization\n",
        "Let's understand the data distribution\n",
        "- Visualize the frequency of word-occurence in all the emails(spam + non-spam)\n",
        "- Visualize the freuency of word-occurence for spam and non-spam emails separately"
      ],
      "metadata": {
        "id": "d0cDiGt6nkPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visuallze data distribution\n",
        "\n",
        "\n",
        "def data_vis(label):\n",
        "  left = [1, 2]\n",
        " \n",
        "  # heights of bars\n",
        "  \n",
        "  height = [label.count('1'),label.count('0')]\n",
        "  \n",
        "  # labels for bars\n",
        "  tick_label = ['spam','ham']\n",
        "  \n",
        "  # plotting a bar chart\n",
        "  plt.bar(left, height, tick_label = tick_label,\n",
        "          width = 0.8, color = ['red', 'green'])\n",
        "  \n",
        "  # naming the x-axis\n",
        "  plt.xlabel('x - axis')\n",
        "  # naming the y-axis\n",
        "  plt.ylabel('y - axis')\n",
        "  # plot title\n",
        "  plt.title('My bar chart!')\n",
        "  \n",
        "  # function to show the plot\n",
        "  plt.show()\n",
        "label=get_label()\n",
        "data_vis(label)"
      ],
      "metadata": {
        "id": "4vJLcTxDLZsh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b24528cf-fb4b-4602-fb9b-e18756487cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learn a Classifier\n",
        "Split the dataset randomly in the ratio 80:20 as the training and test dataset. Use only training dataset to learn the classifier. No test data should be used during training. Test data will only be used during evaluation.\n",
        "\n",
        "Now let us try to use ML algorithms to classify emails as spam or non-spam. You are supposed to implement [SVM](https://scikit-learn.org/stable/modules/svm.html) and [K-Nearest Neighbour](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) algorithm available in scikit-learn using the same training dataset for both."
      ],
      "metadata": {
        "id": "LPq-3nTzGcdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import model_selection \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "# split dataset\n",
        "def split(data):\n",
        "  train_data,test_data=model_selection.train_test_split(feature_sets,test_size=0.25,random_state=seed)\n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "# learn a SVM model\n",
        "# use the model to make prediction\n",
        "# return the model predictions on train and test dataset\n",
        "def svm_classifier(train_data,test_data):\n",
        "  nltk_model=SklearnClassifier(SVC())\n",
        "  nltk_model.train(train_data)\n",
        "  tf,labe=zip(*test_data)\n",
        "  prediction=nltk_model.classify_many(tf)\n",
        "  return classification_report(labe,prediction)\n",
        "\n",
        "# implement k-NN algorithm\n",
        "# use the model to make prediction\n",
        "# return the model predictions on train and test dataset\n",
        "def knn_classifier(train_data,test_data):\n",
        "  nltk_model=SklearnClassifier(KNeighborsClassifier())\n",
        "  nltk_model.train(train_data)\n",
        "  tf,labe=zip(*test_data)\n",
        "  prediction=nltk_model.classify_many(tf)\n",
        "  return classification_report(labe,prediction)\n",
        "\n",
        "train_data, test_data = split(data)\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(svm_classifier(train_data, test_data))\n",
        "print(knn_classifier(train_data, test_data))\n"
      ],
      "metadata": {
        "id": "IMpuaPCKHxwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7645c340-2e7e-4535-f8c3-15c1bbe1f0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2249\n",
            "750\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99       638\n",
            "           1       1.00      0.84      0.91       112\n",
            "\n",
            "    accuracy                           0.98       750\n",
            "   macro avg       0.99      0.92      0.95       750\n",
            "weighted avg       0.98      0.98      0.98       750\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.85      0.90       638\n",
            "           1       0.48      0.79      0.59       112\n",
            "\n",
            "    accuracy                           0.84       750\n",
            "   macro avg       0.72      0.82      0.75       750\n",
            "weighted avg       0.89      0.84      0.85       750\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "Compare the SVM and k-NN model using metrics\n",
        "- Accuracy\n",
        "- [AUC score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)\n"
      ],
      "metadata": {
        "id": "SlCwYvluHqQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "# compute accuracy \n",
        "def compute_accuracy(train_data,test_data):\n",
        "  \n",
        "  nltk_model=SklearnClassifier(KNeighborsClassifier())\n",
        "  nltk_model.train(train_data)\n",
        "  sltk_model=SklearnClassifier(SVC())\n",
        "  sltk_model.train(train_data)\n",
        "  accuracy=nltk.classify.accuracy(nltk_model,test_data)*100\n",
        "  accuracy1=nltk.classify.accuracy(sltk_model,test_data)*100\n",
        "  return accuracy1,accuracy\n",
        "\n",
        "# compute AUC score \n",
        "def compute_auc(train_data,test_data):\n",
        "\n",
        "  nltk_model=SklearnClassifier(KNeighborsClassifier())\n",
        "  nltk_model.train(train_data)\n",
        "  tf,labe=zip(*test_data)\n",
        "  prediction=nltk_model.classify_many(tf)\n",
        "  sltk_model=SklearnClassifier(SVC())\n",
        "  sltk_model.train(train_data)\n",
        "  tf1,labe1=zip(*test_data)\n",
        "  prediction1=sltk_model.classify_many(tf1)\n",
        "  return accuracy_score(labe1,prediction1),accuracy_score(labe,prediction)\n",
        "  \n",
        "  \n",
        "print(compute_accuracy(train_data,test_data))\n",
        "print(compute_auc(train_data,test_data))\n",
        "# write code to print train and test accuracy and AUC score of SVM and k-NN classifier"
      ],
      "metadata": {
        "id": "X3KmcTpqKbGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79b75bb-b1ae-4898-aae1-e140727cb9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(97.6, 84.0)\n",
            "(0.976, 0.84)\n"
          ]
        }
      ]
    }
  ]
}